#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›´æ–°MMDetectioné…ç½®æ–‡ä»¶ä»¥ä½¿ç”¨è½¬æ¢åçš„COCOæ ¼å¼æ•°æ®
"""

import os
import json
from pathlib import Path


def update_mask_rcnn_config():
    """æ›´æ–°Mask R-CNNé…ç½®æ–‡ä»¶"""
    
    # æ£€æŸ¥COCOæ•°æ®æ˜¯å¦å­˜åœ¨
    coco_data_root = "./data/coco_format"
    dataset_info_file = os.path.join(coco_data_root, "dataset_info.json")
    
    if not os.path.exists(dataset_info_file):
        print(f"âŒ æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {dataset_info_file}")
        print("è¯·å…ˆè¿è¡Œ: python convert_annotations_to_coco.py")
        return False
    
    # è¯»å–æ•°æ®é›†ä¿¡æ¯
    with open(dataset_info_file, 'r') as f:
        dataset_info = json.load(f)
    
    print(f"âœ… è¯»å–æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ•°æ®æ ¹ç›®å½•: {dataset_info['data_root']}")
    print(f"  ç±»åˆ«æ•°é‡: {dataset_info['num_classes']}")
    print(f"  è®­ç»ƒæ ‡æ³¨: {dataset_info['ann_file']['train']}")
    print(f"  éªŒè¯æ ‡æ³¨: {dataset_info['ann_file']['val']}")
    
    # åˆ›å»ºæ–°çš„é…ç½®æ–‡ä»¶
    config_content = f'''# Mask R-CNNé…ç½®æ–‡ä»¶ - ä½¿ç”¨è½¬æ¢åçš„COCOæ ¼å¼æ•°æ®
# åŸºäºVOC2012æ•°æ®é›†ï¼Œæ”¯æŒTensorboardå¯è§†åŒ–

# å¯¼å…¥è‡ªå®šä¹‰é’©å­
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from custom_hooks import CustomLoggerHook, TensorboardLoggerHook

# æ¨¡å‹é…ç½®
model = dict(
    type='MaskRCNN',
    data_preprocessor=dict(
        type='DetDataPreprocessor',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        bgr_to_rgb=True,
        pad_mask=True,
        pad_size_divisor=32),
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes={dataset_info['num_classes']},
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes={dataset_info['num_classes']},
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    # è®­ç»ƒé…ç½®
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    # æµ‹è¯•é…ç½®
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))

# æ•°æ®é›†é…ç½®
dataset_type = 'CocoDataset'
data_root = '{os.path.abspath(coco_data_root)}'

# æ•°æ®å¤„ç†ç®¡é“
train_pipeline = [
    dict(type='LoadImageFromFile', backend_args=None),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PackDetInputs')
]

test_pipeline = [
    dict(type='LoadImageFromFile', backend_args=None),
    dict(type='Resize', scale=(1333, 800), keep_ratio=True),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(
        type='PackDetInputs',
        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                   'scale_factor'))
]

# æ•°æ®åŠ è½½å™¨é…ç½®
train_dataloader = dict(
    batch_size=2,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=True),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        ann_file='{dataset_info['ann_file']['train']}',
        data_prefix=dict(img='{dataset_info['img_prefix']}'),
        filter_cfg=dict(filter_empty_gt=True, min_size=32),
        pipeline=train_pipeline,
        backend_args=None))

val_dataloader = dict(
    batch_size=1,
    num_workers=2,
    persistent_workers=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        ann_file='{dataset_info['ann_file']['val']}',
        data_prefix=dict(img='{dataset_info['img_prefix']}'),
        test_mode=True,
        pipeline=test_pipeline,
        backend_args=None))

test_dataloader = val_dataloader

# è¯„ä¼°å™¨é…ç½®
val_evaluator = dict(
    type='CocoMetric',
    ann_file=data_root + '/{dataset_info['ann_file']['val']}',
    metric=['bbox', 'segm'],
    format_only=False,
    backend_args=None)

test_evaluator = val_evaluator

# è®­ç»ƒé…ç½®
train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=40, val_interval=1)
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')

# ä¼˜åŒ–å™¨é…ç½®
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001))

# å­¦ä¹ ç‡è°ƒåº¦å™¨
param_scheduler = [
    dict(
        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),
    dict(
        type='MultiStepLR',
        begin=0,
        end=40,
        by_epoch=True,
        milestones=[30, 37],
        gamma=0.1)
]

# é»˜è®¤é’©å­é…ç½®
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=50),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(
        type='CheckpointHook', 
        interval=1,
        save_best='auto',
        rule='greater'
    ),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    visualization=dict(type='DetVisualizationHook'))

# è‡ªå®šä¹‰é’©å­é…ç½®
custom_hooks = [
    CustomLoggerHook(interval=50),
    TensorboardLoggerHook(interval=50)
]

# ç¯å¢ƒé…ç½®
env_cfg = dict(
    cudnn_benchmark=False,
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='nccl'),
)

# å¯è§†åŒ–é…ç½®
vis_backends = [
    dict(type='LocalVisBackend'),
    dict(
        type='TensorboardVisBackend',
        save_dir='work_dirs/mask_rcnn_coco/tensorboard_logs'
    )
]

visualizer = dict(
    type='DetLocalVisualizer', 
    vis_backends=vis_backends, 
    name='visualizer'
)

# æ—¥å¿—é…ç½®
log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)
log_level = 'INFO'
load_from = None
resume = False

# ç±»åˆ«åç§°ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
metainfo = dict(
    classes={tuple(dataset_info['classes'])},
    palette=None
)
'''

    # ä¿å­˜é…ç½®æ–‡ä»¶
    config_file = "configs/mask_rcnn_coco.py"
    Path("configs").mkdir(exist_ok=True)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    print(f"âœ… Mask R-CNNé…ç½®æ–‡ä»¶å·²æ›´æ–°: {config_file}")
    
    return config_file


def update_train_script():
    """æ›´æ–°è®­ç»ƒè„šæœ¬ä»¥æ”¯æŒæ–°çš„é…ç½®"""
    
    # è¯»å–ç°æœ‰çš„è®­ç»ƒè„šæœ¬
    train_script = "train_models.py"
    
    if not os.path.exists(train_script):
        print(f"âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨: {train_script}")
        return False
    
    with open(train_script, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å¤‡ä»½åŸå§‹æ–‡ä»¶
    backup_file = train_script + '.backup_coco'
    with open(backup_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    # ä¿®æ”¹é…ç½®æ–‡ä»¶è·¯å¾„
    updated_content = content.replace(
        'config_file = "configs/mask_rcnn_voc.py"',
        'config_file = "configs/mask_rcnn_coco.py"'
    )
    
    # æ›´æ–°å·¥ä½œç›®å½•
    updated_content = updated_content.replace(
        'work_dir = "work_dirs/mask_rcnn"',
        'work_dir = "work_dirs/mask_rcnn_coco"'
    )
    
    # æ›´æ–°æµ‹è¯•å‡½æ•°ä¸­çš„é…ç½®æ–‡ä»¶è·¯å¾„
    updated_content = updated_content.replace(
        'test_model("configs/mask_rcnn_voc.py"',
        'test_model("configs/mask_rcnn_coco.py"'
    )
    
    # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
    with open(train_script, 'w', encoding='utf-8') as f:
        f.write(updated_content)
    
    print(f"âœ… è®­ç»ƒè„šæœ¬å·²æ›´æ–°: {train_script}")
    print(f"ğŸ“‹ å¤‡ä»½æ–‡ä»¶: {backup_file}")
    
    return True


def verify_setup():
    """éªŒè¯è®¾ç½®æ˜¯å¦æ­£ç¡®"""
    print("\nğŸ” éªŒè¯è®¾ç½®...")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        "./data/coco_format/annotations/instances_train2012.json",
        "./data/coco_format/annotations/instances_val2012.json",
        "./data/coco_format/dataset_info.json",
        "configs/mask_rcnn_coco.py",
        "custom_hooks.py"
    ]
    
    missing_files = []
    for file_path in required_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print("âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶:")
        for file_path in missing_files:
            print(f"  - {file_path}")
        return False
    
    # æ£€æŸ¥å›¾åƒç›®å½•
    images_dir = "./data/coco_format/images"
    if os.path.exists(images_dir):
        image_count = len([f for f in os.listdir(images_dir) if f.endswith('.jpg')])
        print(f"âœ… å›¾åƒç›®å½•: {images_dir} ({image_count} å¼ å›¾åƒ)")
    else:
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {images_dir}")
        return False
    
    # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶
    train_ann = "./data/coco_format/annotations/instances_train2012.json"
    val_ann = "./data/coco_format/annotations/instances_val2012.json"
    
    try:
        with open(train_ann, 'r') as f:
            train_data = json.load(f)
        with open(val_ann, 'r') as f:
            val_data = json.load(f)
        
        print(f"âœ… è®­ç»ƒé›†æ ‡æ³¨: {len(train_data['images'])} å¼ å›¾åƒ, {len(train_data['annotations'])} ä¸ªæ ‡æ³¨")
        print(f"âœ… éªŒè¯é›†æ ‡æ³¨: {len(val_data['images'])} å¼ å›¾åƒ, {len(val_data['annotations'])} ä¸ªæ ‡æ³¨")
        
    except Exception as e:
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
        return False
    
    print("âœ… æ‰€æœ‰è®¾ç½®éªŒè¯é€šè¿‡!")
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ›´æ–°MMDetectioné…ç½®ä»¥ä½¿ç”¨COCOæ ¼å¼æ•°æ®")
    print("=" * 60)
    
    # 1. æ›´æ–°Mask R-CNNé…ç½®
    config_file = update_mask_rcnn_config()
    if not config_file:
        return
    
    # 2. æ›´æ–°è®­ç»ƒè„šæœ¬
    if not update_train_script():
        return
    
    # 3. éªŒè¯è®¾ç½®
    if not verify_setup():
        return
    
    print("\n" + "=" * 60)
    print("ğŸ‰ é…ç½®æ›´æ–°å®Œæˆ!")
    print("=" * 60)
    print("ğŸ“‹ æ›´æ–°å†…å®¹:")
    print("  âœ… åˆ›å»ºäº†æ–°çš„Mask R-CNNé…ç½®æ–‡ä»¶")
    print("  âœ… æ›´æ–°äº†è®­ç»ƒè„šæœ¬")
    print("  âœ… éªŒè¯äº†æ•°æ®å’Œé…ç½®æ–‡ä»¶")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("1. å¼€å§‹è®­ç»ƒ:")
    print("   python train_models.py --model mask_rcnn")
    print()
    print("2. å¯åŠ¨Tensorboardç›‘æ§:")
    print("   python start_tensorboard.py")
    print()
    print("3. åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹è®­ç»ƒè¿›åº¦:")
    print("   http://localhost:6006")
    
    print("\nğŸ“Š é¢„æœŸçš„Tensorboardå¯è§†åŒ–å†…å®¹:")
    print("  ğŸ”¸ Train_Loss/: è®­ç»ƒæŸå¤±æ›²çº¿")
    print("  ğŸ”¸ Val_Loss/: éªŒè¯æŸå¤±æ›²çº¿")
    print("  ğŸ”¸ Val_mAP/: éªŒè¯mAPæ›²çº¿")
    print("  ğŸ”¸ Learning_Rate/: å­¦ä¹ ç‡å˜åŒ–")
    print("  ğŸ”¸ Time/: è®­ç»ƒæ—¶é—´ç»Ÿè®¡")


if __name__ == "__main__":
    main() 